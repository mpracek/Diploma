\documentclass[12pt,a4paper]{amsart}
% ukazi za delo s slovenscino -- izberi kodiranje, ki ti ustreza
\usepackage[slovene]{babel}
\usepackage[cp1250]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{url}
\usepackage{kbordermatrix}
%\usepackage[normalem]{ulem}
\usepackage[dvipsnames,usenames]{color}
%\usepackage{biblatex}
%\addbibresource{zbraniviri.bib}

% ne spreminjaj podatkov, ki vplivajo na obliko strani
\textwidth 15cm
\textheight 24cm
\oddsidemargin.5cm
\evensidemargin.5cm
\topmargin-5mm
\addtolength{\footskip}{10pt}
\pagestyle{plain}
\overfullrule=15pt % oznaci predlogo vrstico


% ukazi za matematicna okolja
\theoremstyle{definition} % tekst napisan pokoncno
\newtheorem{definicija}{Definicija}[section]
\newtheorem{primer}[definicija]{Primer}
\newtheorem{opomba}[definicija]{Opomba}

\renewcommand\endprimer{\hfill$\diamondsuit$}


\theoremstyle{plain} % tekst napisan posevno
\newtheorem{lema}[definicija]{Lema}
\newtheorem{izrek}[definicija]{Izrek}
\newtheorem{trditev}[definicija]{Trditev}
\newtheorem{posledica}[definicija]{Posledica}


% za stevilske mnozice uporabi naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}


% ukaz za slovarsko geslo
\newlength{\odstavek}
\setlength{\odstavek}{\parindent}
\newcommand{\geslo}[2]{\noindent\textbf{#1}\hspace*{3mm}\hangindent=\parindent\hangafter=1 #2}


% naslednje ukaze ustrezno popravi
\newcommand{\program}{Finanèna matematika} % ime studijskega programa: Matematika/Finan"cna matematika
\newcommand{\imeavtorja}{Martin Praèek} % ime avtorja
\newcommand{\imementorja}{izr. prof. dr. Damjan Škulj} % akademski naziv in ime mentorja
\newcommand{\naslovdela}{Skriti markovski modeli v èasovnih vrstah}
\newcommand{\letnica}{2019} %letnica diplome


% vstavi svoje definicije ...




\begin{document}

% od tod do povzetka ne spreminjaj nicesar
\thispagestyle{empty}
\noindent{\large
UNIVERZA V LJUBLJANI\\[1mm]
FAKULTETA ZA MATEMATIKO IN FIZIKO\\[5mm]
\program\ -- 1.~stopnja}
\vfill

\begin{center}{\large
\imeavtorja\\[2mm]
{\bf \naslovdela}\\[10mm]
Delo diplomskega seminarja\\[1cm]
Mentor: \imementorja}
\end{center}
\vfill

\noindent{\large
Ljubljana, \letnica}
\pagebreak

\thispagestyle{empty}
\tableofcontents
\pagebreak

\thispagestyle{empty}
\begin{center}
{\bf \naslovdela}\\[3mm]
{\sc Povzetek}
\end{center}
% tekst povzetka v slovenscini
V povzetku na kratko opiši vsebinske rezultate dela. Sem ne sodi razlaga organizacije dela -- v katerem poglavju/razdelku je kaj, paè pa le opis vsebine.
\vfill
\begin{center}
{\bf Hidden Markov Models in Time Series}\\[3mm] % prevod slovenskega naslova dela
{\sc Abstract}
\end{center}
% tekst povzetka v anglescini
Prevod zgornjega povzetka v anglešèino.

\vfill\noindent
{\bf Math. Subj. Class. (2010):} navedi vsaj eno klasifikacijsko oznako -- dostopne so na \url{www.ams.org/mathscinet/msc/msc2010.html}  \\[1mm]
{\bf Kljuène besede:} skriti markovski modeli, èasovne vrste, sluèajni proces\\[1mm]
{\bf Keywords:}  hidden markov models, time series, stohastic process
\pagebreak


% tu se zacne besedilo seminarja
\section{Uvod}
Skriti markovski modeli so modelacijsko orodje, ki nam omogoèa zelo široko uporabo. V mojem diplomskem seminarju se bom najbolj posvetil uporabi v finanèni analizi.  
V prvem poglavju se bom posvetil razliènim naèinom uporabe skritih markovskih modelov, v drugem pa bom natanèno opisal matematièno ozadje le teh. Na koncu bom natanèno predstavil še lasten primer uporabe, kjer si bom pomagal z razliènimi programskimi jeziki in okolji, od Mathematice do R.



\pagebreak
% slovar
\section*{Slovar strokovnih izrazov}

\geslo{Gaussova mešanica}{Gaussova mešanica je porazdelitvena funkcija z gostoto, ki jo lahko zapišemo kot tehtano povpreèje normalnih gostot; $$f(x) = \sum_{j = 1}^{M}{c_{j}} N(x;\mu_{j}, \sigma_{j}^2)$$. Veljati mora še, da je $$\sum_{j = 1}^{M}{c_{j}} = 1$$ in da je $N(x;\mu_{j}, \sigma_{j}^2)$ gostota normalne porazdelitvene funkcije.} \\
\geslo{Akaikejev informacijski kriterij}{Akaikejev informacijski kriterij oziroma AIC je merilo relativne kakovosti modela glede na preostale modele za iste podatke.	\\Recimo, da imamo podatke s $k$ ocenjenimi parametri in $L$ maksimalna vrednost funkcije verjetja. AIC izraèunamo kot $$AIC = 2k - 2ln(L)$$ AIC primerja veè razliènih modelov po kakovosti, kjer je najboljši tisti z najmanjšo vrednostjo.}\\
\geslo{Baum-Welchov algoritem}{Je algoritem, s katerim poišèemo neznane parametre skritih markovskih modelov.}\\
\geslo{Dinamièna bayesianska mreža}{Je bayesianska mreža, v kateri so povezane spremenljivke, ki ležijo v dveh sosednjih èasovnih korakih.}
%
\pagebreak
\section{Markovski modeli}
%Dodaj še kaj o markovskih modelih
Skriti markovski modeli so ena izmed vrst bolj splošne skupine modelov, ki jo imenujemo markovski modeli. Gre za modele, kjer se stanja sluèajno spreminjajo. Za modele velja, da je njihovo stanje v èasu $t$ odvisno le od stanja v èasu $t-1$. To imenujemo tudi markovska lastnost, po kateri so ti modeli tudi poimenovani.\\

%vprašaj za primerno markovsko lastnost; kakšno primerno trditev, definicijo 

%\begin{trditev}{Markovska lastnost}\\ {Naj velja, da imamo verjetnostni prostor s filtracijo $(\Omega,\mathbb{F},P,\mathbb{F}_{n})_{n\geq1})$. Naj bo $(X_n)_{n\geq1}$ zaporedje neodvisnih enako porazdeljenih sluèajnih spremenljivk ali vektorjev. Naj velja še, da je $T$ èas ustavljanja glede na naravno filtracijo, kjer velja $T<\infty$ skoraj gotovo. Potem je zaporedje $$(X_{T+1},X_{T+2},\ldots) (X_{1},X_{2},\ldots)$$ in je neodvisno od $\sigma$-algebre zgodovine èasa $\mathbb{F}_{T}$.}
%\end{trditev}

%\begin{proof}
%%Naj bo A poljuben dogodek iz $\( \mathcal{F} \)_{T}$. Potem velja:
%$$P(A \cap \{X_{T+1}\leq c_1,X_{T+2}\leq c_1,\ldots,X_{T+n}\leq c_n\}) = $$ 
%$$P(\cup_{l=1}^\infty A \cap \{T=l\} \cap \{X_{T+1}\leq c_1,X_{T+2}\leq c_1,\ldots,X_{T+n}\leq c_n\})=$$
%$$\sum_{l=1}^{\infty}P(A \cap \{T=l\} \cap \{X_{T+1}\leq c_1,X_{T+2}\leq c_1,\ldots,X_{T+n}\leq c_n\})=$$
%$$\sum_{l=1}^{\infty}P(A \cap \{T=l\} \cap \{X_{l+1}\leq c_1,X_{l+2}\leq c_1,\ldots,X_{l+n}\leq c_n\})=$$
%$$\sum_{l=1}^{\infty}P(A \cap \{T=l\}) P(\{X_{l+1}\leq c_1,X_{l+2}\leq c_1,\ldots,X_{l+n}\leq c_n\})=$$
%$$\sum_{l=1}^{\infty}P(A \cap \{T=l\}) P(\{X_{1}\leq c_1,X_{2}\leq c_1,\ldots,X_{n}\leq c_n\})=$$
%$$P(\{X_{1}\leq c_1,X_{2}\leq c_1,\ldots,X_{n}\leq c_n\})\sum_{l=1}^{\infty}P(A \cap \{T=l\}) =$$
%$$P(\{X_{1}\leq c_1,X_{2}\leq c_1,\ldots,X_{n}\leq c_n\})P(\cup_{l=1}^\infty A \cap \{T=l\})$$
%$$P(\{X_{1}\leq c_1,X_{2}\leq c_1,\ldots,X_{n}\leq c_n\})P(A)$$
%\end{proof}
Naravno se nam postavi vprašanje, zakaj je markovska lastnost koristna. Izkaže se, da nam omogoèa reševanje problemov, ki jih drugaèe v primernem èasu ne bi mogli rešiti. Model, ki ga lahko tako rešimo imenujemo Markov model.\\
%Tukaj dopiši še kar nekaj o teh modelih, da se tabela prestavi na naslednjo stran
%Ugotovi napako
\begin{table}[h]
	\begin{tabular}{|c|c|c|}
		\hline
		& V celoti opazovan & Le delno opazovan             \\
		\hline
		{Avtonomen} & Markovska veriga                & Skriti markovski model                      \\
		\hline
		Kontorliran                   & Markovski proces odloèanja      & Delno opazovalen proces odloèanja
		\show \hline
	\end{tabular}
\end{table}\\
%zakljuèi zapis po tabeli, dopiši še kakšno zanimivost.
\subsection{Primer žabe v ribniku}
Zamislimo si, da imamo ribnik z lokvanji, v katerem so se naselile žabe. Vsaka izmed žab je lahko v vsakem trenutku v vodi, na lokvanju ali pa zunaj vode. To imenujemo stanja markovskega modela $S$. Poimenujmo vektor stanj  $$S = (Voda,Lokvanj,Zemlja).$$ Na ribnik pogledamo le v nekih diskretnih èasih, z enakim razmikom, na primer ob koncu vsake minute. Odloèimo se za opazovanje le ene izmed žab, za katero nas zanima, kje se bo nahajala v naslednjem trenutku.\\
Do sedaj smo opazovali splošen primer za vse tipe markovskih modelov. Na tem mestu pa moramo vedeti, ali je naša opazovana žaba dresirana, in se bo odzvala na naše opazovanje. Èe to velja, potem imamo kontroliran markovski model, ki je lahko delno opazovan ali markovski proces odloèanja.\\
Predstavljajmo si, da lahko vsakiè ko pogledamo, našo žabo poljubno premaknemo ali pa pustimo na mestu. Žaba lahko nato poljubno skaèe naprej. Èe poznamo prehodno matriko med stanji bomo v tem primeru imeli- markovski proces odloèanj. V primeru ko pa to ne velja, imamo delno opazovan proces odloèanja.\\
Na drugi strani pa imamo avtonomen model, kjer lahko loèimo èe je proces v celoti opazen ali so neka stanja skrita. Èe stanja niso skrita, tako poznamo prehodno matriko $A$, ki nam pove, s kašno verjetnostjo se premikamo med posamiènimi stanji. V tem primeru imamo markovsko verigo, ki je le poseben primer markovskih procesov odloèanja, kjer žabe nikoli ne prestavimo. Preprost primer markovskih verig predstavljajo sluèajni sprehodi. \\
Recimo, da v našem primeru velja, da je prehodna matrika oblike:
$$
\begin{bmatrix}
		0.5 & 0.3 & 0.2  \\
	0.1 & 0.5 & 0.4  \\
	0.4 & 0.2 & 0.4 \\
\end{bmatrix}$$
Matrika je torej znana, in navadno predpostavimo, da se verjetnosti prehoda iz enega stanja v drugega ne spreminjajo. Vsota po vsakem stolpcu in vrstici je  vedno enaka ena, to je $$\sum_{i = 1}^{n}a_{ij}= \sum_{j = 1}^{n}a_{ij} = 1$$ saj predstavlja verjetnosti prehoda iz enega stanja v drugo. Tako je $a_{ij}$ verjetnost prehoda iz stanja $j$ v stanje $i$. Velja še, da je $n$ število stanj in je v našem primeru enako $3$, oèitno pa velja tudi, da je matrika velikost $n\times n$ in da so vsi elementi $a_{ij}$ iz intervala $[0,1]$.\\
V tem primeru lahko torej, v vsakem trenutku predvidimo, kam bo najverjetneje skoèila žaba v naslednjem trenutku. Na drugi strani pa imamo še skrite markovske modele.\\
Za skrite markovske modele pa velja, da same prehodne matrike naèeloma ne poznamo, jo pa lahko doloèimo iz podatkov, ki jih imamo za naš problem. Tako bi za našo žabo dovolj dolgo opazovali, in si zapisovali njene pozicije glede v vsakem trenutku opazovanja. Tako bi dobili vektor opazovanj $O$. Vektor $O$ je naš osnovni podatek, iz katerega nato izpeljemo celoten model. Iz tega bi najprej pogledali, kakšna so možna stanja sistema. Iz tega bi nato poizkušili ugotoviti, kako izgleda prehodna matrika. Kako to naredimo, si bomo pogledali v nadaljevanju. 

\section{Skriti markovski modeli}
Skriti markovski model je statistièni markovski model, kjer predpostavljamo, da je modelirani sistem markovski proces z skritimi stanji.\\
Gre za najbolj preprosto vrsto dinamiène bayesianske mreže. Tu gre za vrsto bayesianske mreže, v kateri so povezane spremenljivke, ki ležijo v dveh sosednjih èasovnih korakih. Tako lahko vrednost v èasu $t$ izraèunamo prek vrednosti v èasu $t-1$ ter notranih \textbf{regresorjev}.
Gre torej za tip modela, kjer lahko razberemo rezultat, ne moremo pa ugotoviti, kakšna je bila funkcija, ki nam ga je dala. 
%Podaljšaj opis, dodatne razlage. Poišèi še kaj po virih. Dopiši!!

%Bolj podroben opis
%latent markov models

\subsection{Zgodovina modela}
Ideja skritih markovskih modelov se v svoji prvi fazi zaène z ruskim matematikom Andrejem Andrejevièem Markovom, ki je za èasa svojega življenja med $1865-1922$ razvil idejo markovskega procesa in verige. Prve teoretiène rezultate markovskih verig je svetu predstavil $1906$, $1913$ pa je izraèunal zaporedje èrk rušèine. Slednji izraèun je opravil na besedilu Jevgenij Onjegin ruskega pesnika Aleksandra Sergejevièa Puškina. Markov je želel s tem dokazati veljavnost šibkega zakona velikih števil zaradi spora z drugim ruskim matematikom Pavlom Nekrasom, a je na ta naèin odprl novo vprašanje v matematiki.\\
Skriti markovski modeli zahtevajo veliko numeriènega raèunanja, in zato ne preseneèa, da se je nadaljni razvoj zaèel šele z širšo uporabo raèunalnikov. Po pomembnem delu, ki so ga opravili von Neumann, Turing in Conrad Zuse so se znanstveniki zaèeli pospešeno ukvarjati z implementacijo primernih algoritmov. Velik korak k temu je pripomogel Claude Shannon s svojim delom Matematièna teorija komunikacije.\\ 
Za implementacijo skritih markovskih modelov je bilo v zaèetku potrebnih kar nekaj razvojev algoritmov:
\begin{enumerate}
\item Algoritem maksimizacije prièakovanega (Expectation-maximization), ali, kot ga med drugim poznamo, EM-algoritem so uporabljali med drugim že Laplace, Gauss in drugi, vendar je do dokonènega poimenovanja prišlo šele leta $1977$. Demster, Laird in Rubin so algoritem poimenovali in razložili splošno teorijo, ki deluje v ozadju procesa.
\item \label{Viterbi}Andrew Viterbi je drugi pomemben del skritih markovskih modelov implementiral leta $1967$ kot algoritem za dekodiranje konvolucij èez šume v komunikaciji. Pri viterbijevem algoritmu gre za algoritem iz dinamiènega programiranja, ki najde najbolj verjeto zaporedje stanj, glede na dano opazovano zaporedje dogodkov.
\end{enumerate}
Celotnih modelov pa prav gotovo ne bi bilo brez še enega Rusa, in sicer Ruslana Leontijevièa Stratonovièa, ki je prvi opisal rekurzijo naprej - nazaj leta $1960$. To je delal na primeru optimalnega nelinearnega problema filtracije.\\
Za glavnega avtorja pa velja Leonard E. Baum. Skupaj z  Lloyd R. Welchem sta okrog leta $1970$ razvila Baum-Welchov algoritem, kjer gre za poseben primer posplošenega EM-algoritma. Prav Baum-Welchov algoritem je tisti korak, pri katerem velja, da je bila dokonèno izpeljana teorija za skritimi markovskimi modeli. Algoritem uporablje EM-algoritem da najde cenilko po metodi najveèjega verjetja glede na dano zaporedje podatkov.



\subsection{Zahteve za model}
Kot vsak matematièni model, ima tudi skriti markovski model svoje zahteve.
\begin{enumerate}
	\item Prva zahteva je, da lahko rezultate, ki jih model producira, opazujemo v ekvidistanènih èasih, torej je razlika med dvema poljubnima zaporednima èasoma opazovanja $t-1$ in $t$ vedno enaka, na primer $d$. Rezulate imenujemo signali ali opazovanja.
	\item V vsakem izmed èasov $t$ je sistem lahko v enem izmed $N$ stanj. Ta stanja so $S_{1}, S_{2}, \ldots, S_{N}$. Vsako izmed stanj $S_{i}$ je sluèajna spremenljivka, ki je lahko zvezna ali diskretna, a njenega porazdelitvenega zakona ne poznamo. Ta stanja so v èasu $t$ neznana.
	\item  V vsakem izmed èasov $t$ ne vemo, v kakšnem stanju se nahajamo, vemo le, kakšni so rezultati našega procesa v tem èasu. Zato potrebujemo dodatni sluèajni proces ${\displaystyle Q=(Q_{t})_{t = 1,2, \ldots }}$, ki nam pove, v kakšnem stanju je sistem v èasu $t$. Tako velja, da je signal $O_{t}$ dan s stanjem sluèajnega procesa $Q$, v odvistnosti od gostote verjetnostne porazdelitve Gaussove mešanice. Signali našega procesa so torej rezultati sluèajnega procesa $Q$.
	\item Vektor verjetnosti zaèetnih stanj je oznaèen z $\Pi$, in vsota njegovih elementov je enaka $1$.
	\item V vsakem èasu velja, da bodisi sistem spremeni svoje stanje, bodisi ostane isto. Verjetnost prehoda v vsako stanje je doloèena s prehodnimi verjetnostmi, podanimi v matriki $A^{t}$, kjer verjetnost prehoda iz stanja $i$ v èasu $t$ v stanje $j$ v èasu $t+1$ simbolizira element $a_{ij}^{t}$. 
	\item Vsota vsakega stolpca vsake matrike $A^{t}$ je enaka $1$.
	\item  Ker govorimo o markovskem modelu, bo stanje v èasu $t+1$ odvisno le od stanja v èasu $t$, ne pa od celotne zgodovine.
\end{enumerate}
 \ \\
S temi predpostavkami bi lahko zmodelirali, a jih imamo še nekaj, ki nam sam model še precej olajšajo.
\begin{enumerate}
	\item Predpostavimo lahko, da so vse prehodne matrike $A^{t}$ enake, torej neodvisne od èasa $t$. Enolièno prehodno matriko torej lahko oznaèimo $A$. 
	\item Signal v èasu $t$ je odvisen le od stanja modela v tem èasu; torej so sluèajne spremenljivke v èasu $t$ odvisne le od tega.
\end{enumerate}
 \  \\
Naši signali so torej odvisni od stanja $S$. Ta stanja nam vrnejo rezultat v odvisnosti od verjetnostnih porazdelitev podanih z razliènimi parametri. Le te predstavimo kot mešanice normalnih porazdelitev in jih imenujemo tudi Gaussove mešanice.
Gaussovo mešanico definiramo kot tehtano povpreèje normalnih porazdelitvenih funkcij, glej slovar strokovnih izrazov. Vsako stanje $i$ iz nabora vseh stanj je torej podano z porazdelitveno gostoto $$b_{i} = \sum_{j = 1}^{M}{c_{ij}} N(x;\mu_{j}, \sigma_{j}^2)$$.\\
\\
Gaussove mešanice so primerne, ker lahko zelo dobro aproksimirajo vsako konèno zvezno porazdelitev, poleg tega pa se znebimo tveganja, ki nam ga v tem modelu predstavlja normalna porazdelitev. To tveganje predstavlja dejstvo, da je normalna porazdelitev simetrièna; to pa pogosto ne drži za procese v realnem svetu, na primer za donose v financah. Za te veèkrat drži, da so "left-skewed", torej da je veèji del manjši od povpreèja. \\
Tako potrebujemo za sestavo modela še:
\begin{itemize}
	\item število mešanic $M$
	\item matriko $C$, ki predstavlja koeficiente $c_{ij}$, ki so faktorji v Gaussovi mešanici,
	\item matrika $\Gamma$, kjer $\mu_{ij}$ predstavlja prièakovano vrednost mešanice $j$ v stanju $i$,  ter
	\item matriko $\Sigma$, kjer $\sigma_{ij}$ predstavlja varianco mešanice $j$ v stanju $i$.
\end{itemize}
\   \\
\subsection{Priprava in trening modela}\label{trening}
Ko vemo kaj potrebujemo, lahko zaènemo s pripravo naših modelov. Preden zaènemo s doloèanjem parametrov našega modela, moramo najprej dobro preuèiti naš problem, saj je to znanje kljuèno za dobro reševanje problema. Kot prvo stvar moramo najprej izbrati set podatkov, na katerem se bo potekal tako imenoval trening sistema. 
%Popravi, najraje na dejanskem isti enoti.
Ti podatki morajo biti zbrani na na podobni enoti; èe na bo na primer zanimale cena delnice podjetja, ki se ukvarja z predelavo pomaranènega soka, za trening ne bomo vzeli cene delnic metalurškega podjetja. 
\\ Podatke, ki smo jih zbrali moramo nato urediti; doloèimo èasovne trenutke z enakim razponom, kot ga želimo imeti v našem modelu, in v teh trenutkih $t \in (1,\ldots, T)$ doloèimo opazovanja $O$. 
\\ Ko imamo podatke zbrane, moramo najprej doloèiti število stanj $N$ ter število mešanic $M$. Tu gre tudi za kljuèna problema priprave skritega markovskega modela.
\\V praksi število stanj vèasih doloèimo z potrebno aplikacijo, kot na primer v \ref{Biologija}, oziroma z vizualnim ogledom toèk grafa iz zgodovinskih podatkov.\\
Vèasih pa lahko to doloèimo preko Akaikejevega informacijskega kriterija, ki primerja veè razliènih modelov, ki lahko doloèijo $N$ in izbere najboljšega. 
%Dopiši še kaj o Akaikejevem kriteriju, omeni še Bayesianski kriterij
\\ Za pravilno doloèitev števila mešanic $M$ pa navadno izberemo razvršèanjem v skupine ($k$-mean clustering). 
%Zakaj, kako to naredimo?
\\
Ostale parametre navadno oznaèimo z $\lambda$ $=$ $(\Pi,A,C,\Gamma,\Sigma)$. Cilj treninga modela je, da parametre nastavimo tako, da bo verjetnost, da so bila vsa opazovanja doloèena tudi s strani modela najveèja, $P(O|\lambda)$. Pri tem mora veljati, da sta število mešanic $M$ ter število stanj $N$ že znana.
\\Naravno se pojavi vprašanje, kako to doloèimo. Izkaže se, da je uèinkovit sistem $naprej - nazaj$. Doloèimo spremenljivki $naprej$ $\alpha_{t}(i)$, ki predstavlja verjetnost, da so se zgodila opazovanja od $o$ do $t$ in stanje $i$, ter $nazaj$ $\beta_{t}(i)$, ki predstavlja verjetnost, da se bodo zgodila opazovanja od $t$ do $T$ pri stanju $i$. Definirati pa rabimo le eno izmed njiju, saj gre pri $\beta_{t}(i)$ le alternativno metodo za $\alpha_{t}(i)$.
$\alpha_{t}(i)$ definiramo kot:
$$\alpha_{1}(i) = \pi_{i}b_{i}(O_{1}) = \pi_{i}b_{i} = \sum_{k = 1}^{M}{c_{ik}} N(O_{1};\mu_{j}, \sigma_{j}^2)$$
Naslednje $\alpha_{t+1}(j)$ lahko nato induktivno izraèunamo kot 
$$\alpha_{t+1}(j) = b_{j}(O_{t+1})\sum_{i=1}^{N}{\alpha_{t}(i)a_{ij}}$$
Iz tega sledi, da je $P(O|\lambda) = \sum_{i=1}^{N}{\alpha_{T}(i)}$. Na enak naèin izraèunamo tudi $\beta_{t}(i)$, ki ni nujen za izraèun  $P(O|\lambda)$, vendar ga nujno rabimo za trening našega modela.
\\
\ \\
Skozi celotno toèko \ref{trening} smo se obnašali, kot da lahko vse podatke iz $\lambda$ kar izraèunamo. Vendar to ne drži v popolnosti. Res, ko je enkrat $naprej-nazaj$ vzpostavljen, se nam dozdeva, da gre le še za numerièno operacijo. Vendar težave nastopijo preden naš postopek zaène z delom. Problem nastopi ko moramo v naprej oceniti zaèetne parametre $\lambda$. \\
Ne poznamo analitiènega naèina, kako bi lahko ta problem rešili, vendar lahko najdemo tak $\lambda$, da lahko našo verjetnost $P(O|\lambda)$ lokalno maksimiziramo.Tu lahko uporabimo razliène algoritme, najbolj pa je uporabljen Baum-Welchov.\\
%Opiši BW
Pri tem moramo vsak paramater $\lambda$ najprej pogledati posebej. Izkaže se, da zaèetni vrednosti za prehodno matriko $A$ in zaèetni vektor $\Pi$ nista pomembni, èe nista ravno nièelna. Za $\Pi$ tako lahko vzamemo vektor, kjer so vse vrednosti enake $1/N$, kjer je $N$ število stanj.
\\
Veè problemov nam povzroèajo tri postavke z zvezno porazdelitivjo, to je $C$, $\Sigma$ in $\Gamma$. Dobra zaèetna ocena le teh je nujna za kakovost modela. Tudi tu se izkaže, da lahko, podobno kot pri oceni števila mešanic $M$ ozremo na razvršèanjem v skupine. Ta postopek nam sicer ne da globalnega minimuma, ki bi ga mogoèe lahko dobili na drugaèen naèin, a se v praksi izkaže za dobrega. Za $C$ to pomeni, da bo element $c_{ij}$ $=$ $1/k$ za vsak par $ij$, kjer je $k$ število šopov povpreèij. Prièakovane vrednosti in variance nato pridobimo iz vrednosti šopov povpreèij.
\ \\
Z znanimi zaèetnimi vrednostmi se lahko spustimo v maksimiziranje $P(O|\lambda)$. Osnovati moramo zaporedje $\lambda$ = $\lambda_{t}, t \in {0,\ldots}$, da bo veljalo  $$P(O|\lambda_{i+1})  \geqslant P(O|\lambda_{i})$$
Za to zaporedje velja, da konvergira k lokalnemu maksimumu.\\

\subsection{Natanèna doloèitev zaèetnih vrednosti}
Zanima nas torej, kako bomo osnovali zaporedje $\lambda$, ki ga potrebujemo za maksimizacijo $P(O|\lambda)$. Izkaže se, da za maksimizacijo le tega potrebujemo še nekaj dodatnih formul, ki so rezultat Baum-Welchovega algortma. Z njimi definiramo veèfazni iterativni proces, pri katerem popravljamo vrednosti parametrov do konvergence.\\
Kot prvo potrebujemo $\xi_{t}(i,j)$, ki nam pove verjtnost, da smo v èasu $t$ v stanju $i$ in v èasu $t+1$ v stanju $j$.
$$\xi_{t}(i,j) = \frac{\alpha_{t}(i)a_{ij}b_{j}(O_{t})\beta_{t+1}(j)}{P(O|\lambda)}.$$
Vsota $\sum_{t=1}^{N}{\xi_{t}}$ nam pove prièakovano število prehodov iz stanja $i$ v stanje $j$.\\
Druga je $\gamma_{t}(i)$, ki nam pove verjetnost, da smo v èasu $t$ v stanju $i$.
$$\gamma_{t}(i) = \frac{\alpha_{t}(i)\beta_{t}(i)}{P(O|\lambda)}$$
Vsota $\sum_{t=1}^{N}{\gamma_{t}(i)}$ nam pove prièakovano število prehodov iz stanja $i$.\\
Zadnja, tretja pa je $\gamma_{t}(j,k)$, ki nam pove verjetnost, da smo v èasu $t$ v stanju $j$, in $k$ta mešanica predstavlja $O_{t}$. 
$$ \gamma_{t}(j,k) =\gamma_{t}(j)\frac{c_{jk} N(x;\mu_{jk}, \sigma_{jk}^2)}{\sum_{m = 1}^{M}{c_{jm}} N(x;\mu_{jm}, \sigma_{jm}^2)}$$
\ \\
Iz teh podatkov lahko nato popravimo izraèun elementov:
$$ \overline{\Pi_{i}} = \gamma_{1}(i)$$
$$ \overline{a_{ij}} = \frac{\sum_{t=1}^{T-1}{\xi_{t}}}{\sum_{t=1}^{T-1}{\gamma_{t}(i)}}$$
$$\overline{c_{jk}}= \frac{\sum_{t=1}^{T}{\gamma_{t}(j,k)}}{\sum_{t=1}^{T}\sum_{m=1}^{M}{\gamma_{t}(j,m)}}$$
$$\overline{\mu_{jk}}=\frac{\sum_{t=1}^{T}{\gamma_{t}(j,k)}O_{t}}{\sum_{t=1}^{T}{\gamma_{t}(j,k)}} $$
Ko je to doloèeno, nam zaporedje $\lambda$, doloèenih s temi parametri da lokalni maksimum. 
%Kako nam da? konvergenca? kakšna? 
\subsection{Trenutno stanje}
Zadnja stvar, ki jo moramo narediti, preden zaènemo z doloèanjem prihodnjih stanj je doloèitev trenutnega stanja gospodarstva, torej stanja v zadnjem èasu, na katerem je naš model treniral.
Za doloèitev le tega uporabimo tako imenovani Viterbijev algoritem. Le ta je oblikovan tako, da nam vrne zaporedje $Q$, ki maksimizira $P(Q|O,\lambda)$.\\
Za delo s tem algoritmom moramo definirati $\delta_{t}(i)$, ki za vsako stanje $i$ vrne najveèjo verjetnost vzdolž poti v èasu $t$. Prek $\delta_{t}(i)$ nato induktivno izvedemo algoritem.\\
Viterbijev algoritem nam vrne $p*$, ki je najveèja verjetnost in $q_{T}*$, ki nam pove stanje v èasu $T$, ki nam to verjetnost vrne.
\pagebreak
\section{Uporaba}
%Popravi naslednji stavek
%razloži zakaj široka uporabnost
Skriti markovski modeli so zelo široko uporabno orodje za modeliranje. Naèini uporabe se zelo razlikujejo, od uporabe v financah do doloèevanja genoma. Ekonomskim, torej tistim, ki jih delujejo kot napovedovalci cen vrednostnih papirjev v prihodnosti, se bom najglobje posvetil v nasledenjem delu, zato sedaj raje poglejmo ostale naèine uporabe.

%dodajaj slike èe možno

\subsection{Procesiranje govora}
Prva ideja Leonarda E. Bauma za njegov algoritem je bila prav uporaba v procesiranju govora.
Ena najbolj široko uporabljenih naèinov uporabe pa je procesiranje govora za posamiène glasovne enote. Gre za sistem, kjer želimo prepoznati posamiène izgovorjene besede, ne moremo pa ga uporabiti za splošen govor, saj algoritem ni implementiran za "zvezen" govor. Ideja tega algoritma je razviti najboljše možne aproksimacijske algoritme, da lahko skriti markovski model filtrira nakljuène šume, zvoke na najboljši možen naèin. \\
Da bomo procesiranje lahko zmodelirali moramo najprej doloèiti slovar glasov, iz katerega vemo, da bo glas prišel. Tu uporabljam besedo glas, ker ni nujno, da bo prepoznan glas dejansko beseda; eden izmed glasov bi lahko bil tudi zgolj èrka A.
Velja omeniti tudi, da na ta naèin, le z nekaj adaptacijami, deluje tudi Siri.
%kje se ga še uporablja? Kako deluje
%dejanski primer


\subsection{Uporaba v biologiji in biokemiji} \label{Biologija}
%Uporaba subsubsectionov
Gre za eno izmed prvih uporab skritih markovskih modelov. Ker se je uporaba na tem podroèju zaèela precej zgodaj tudi ne preseneèa, da je uporaba precej široka.
%pomagaj si z nežo
%mogoèe razdeli na veè toèk - genetika, procesi v celièni membrani
%dejanski primer
%zakaj zanimivo, pomembno

V zadnjem èasu se skrite markovske modele vedno veè uporablja tudi za modeliranje bioloških in biokemijskih procesov, med katerimi je najbolj  znan primer modeliranja proteinov v celièni membrani, ki ga bom malo opisal, poleg tega pa med drugim še za {\bf napovedovanje genov?}. \\
Èeprav lahko nekatere proteine in njihovo delovanje znotraj celiène membrane enostavno predstavimo, to ne velja za vse. Tako imenovani $\beta$
-\textbf{barrel} membranski proteini zahtevajo veè dela. Da bi ugotovili njihovo delovanje in ga primerjali z delovanjem v vodi topnih proteinov so Bagos, Liakoupos et al. razvili model ki to naredi. \\
Z delom potem nadaljujemo podobno kot pri procesiranju govora, le da je naš zaèetni slovar tu dolg $20$ znakov, toliko kolikor je aminokislin.
\\
\subsection{Napovedovanje prevoza}

Gre za sistem, kjer želimo za prihodnost napovedati, koliko vozil bo neko prometno infrastrukturo v prihodnosti.
%zakaj zanimivo, pomembno
%Preberi še kakšen vir
%Razloži probleme le tega, primer
%dejanski primer


\subsection{Prepozavanje lastnoroène pisave}
%dodatni viri
%zakaj zanimivo, pomembno
%dejanski primer


\subsection{Kriptoanaliza}
S tem se je ukvarjajo razlièni storokovnjaki.
%zakaj zanimivo, pomembno
%novi viri, zbrani, preberi
%dejanski primer

\subsection{Prepoznavanje akcij}
%zakaj zanimivo, pomembno
%dejanski primer

\pagebreak
\section{Èasovne vrste}
%razloži še kaj o èasovnih vrstah iz virov dopiši kaj
%time analysis
\definicija{Èasovna vrsta} množica opazovanj $x_t$, vsako opazovano ob èasih $t$ znotraj nekega èasovnega intervala.\\
\ \\ 
Opazovanja $x_t$ so razporejena po èasih, ko so bila zaznana, torej narašèajoèem èasu $t$. V realnosti pogosto velja, da so zaporedna opazovanja med seboj odvisna. To je predvsem oèitno, èe pomislimo na najbolj tipièen primer finanène èasovne vrste, in sicer vrednostnega procesa cene neke delnice.  Èe vidimo, da ima v èasu $t$ delnica ceno $p$, si lahko navadno mislimo, da bo v èasu $t+1$ cena blizu $p$.
Glede na èase, v katerih gledamo rezultate naše èasovne vrste v grobem loèimo $3$ tipe:
\begin{itemize}
	\item Èasovne vrste v zveznih èasih. Tu pridobivamo podatke za vsak trenutek znotraj nekega èasovnega intervala. Dober primer je tu EKG, kjer v vsakem trenutku preiskave naprava izrisuje graf elektriène napetosti proti èasu.
	\\ V analizi finanènih èasovnih vrst za ta tip uporabljamo Black-Scholesov model. Gre za podvrsto paraboliène diferencialne enaèbe, katere rešitev nam da predvideno vrednost evropskih opcij $call$ in $put$ v èasu, ki nas zanima.   
	\item Èasovne vrste z diskretnimi vrednostmi kjer  pridobivamo vrednosti, ki lahko zavzamejo le diskretne vrednosti, torej najveè števno neskonèno razliènih možnosti.\\ Tipièen primer bi bil število klicev med $7.00$ in $8.00$ znotraj Ljubljane oziroma število nesreè znotraj enega dneva na avtocesti med Postojno in Ljubljano. 
	\item Zadnja vrsta so diskretne èasovne vrste, torej vrste, ki vrednosti vzamejo le v doloèenih èasovnih trenutkih, npr. konec vsake ure, konec dneva, \ldots Tukaj lahko vrednosti zavzamejo kakršnokoli vrednost, pomembno je le, da te vrednosti vzamemo ob pravilnih trenutkih. Te vrste so uporabne tudi za aproksimiranje èasovnih vrst v zveznih èasih. Prav vrste takega tipa so tiste, ki jih lahko analiziramo z skritimi markovskimi modeli.
\end{itemize}
S finanènimi vrstami se sreèujemo vsak dan in v najrazliènejših oblikah. Zaradi njihove raznolikosti je izbira pravega modela za njihovo analizo kljuèna. 
\definicija{Model èasovne vrste} za opazovane podatke ${x_t}$ je sluèajni proces $X_t$, kjer velja, da so $x_t$ realizacije tega sluèajnega procesa v èasih $t$.\\
\ \\
V našem primeru bomo za njihovo analizo uporabili skrite markovske modele. \\
%mogoèe še kaj o èasovnih vrstah
%definicija finanène èasovne vrste
Finanèna èasovna vrsta je tako zaporedje opaženih vrednosti nekega finanènega instrumenta. 
Analiza finanènih èasovnih vrst se ukvarja z teorijo in prakso doloèanja vrednosti finanènih instrumentov skozi èas. Zaradi elementa negotovosti, kot je na primer volatilnost finanènih instrumentov, jo moramo obravnavati loèeno od vseh ostalih èasovnih vrst. Velja namreè da ravno ta negotovost prikaže pomembnost statistiène teorije in metod, izpeljanih iz le te, v analizi finanènih èasovnih vrst.
%kako HMM apliciramo v èasovne vrste
%posebnosti finanènih èasovnih vrst

\section{Uporaba skritih markovskih modelov v finanènih èasovnih vrstah}
%iz prvega vira o CVar izpelji kaj
%iz drugega vira razloži primere, kaj delajo
%kaj lahko delamo z HMM
%uporaba v optimalnih trgovalnih strategijah -> zaporedna optimizacija portfelja


\pagebreak
\section{Praktièni primer}
\subsection{Analiza kakovosti modela}
%slika podatkov
%psevdokoda/slika kode
%ekonometrièna primerjava modela z dejanskimi vrednostmi
% v tem delu bomo ugotavljali prileganje modela z podatki iz zgodovine z ostalimi podatki iz zgodovine
%vpliv spreminjanja kriterijev na toènost
%primerjal bom zbrane vrnjene vrednosti z dejanskimi vrednostmi
%delal bom na nekaj izbranih primerih, le nekaj vrednostnih papirjev
%kriptovalutni bum in padec
%borzni zlom 2008, 1929
%dogajanje okoli 2005 



%pogledam kakovost prek ekonometriènih testov;

\subsection{Napoved}
%zlato, srebro
%kriptovalute



\pagebreak
% seznam uporabljene literature
\begin{thebibliography}{99}
\bibliography{zbraniviri}
\end{thebibliography}

\end{document}

