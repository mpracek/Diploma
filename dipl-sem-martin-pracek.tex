\documentclass[12pt,a4paper]{amsart}
% ukazi za delo s slovenscino -- izberi kodiranje, ki ti ustreza
\usepackage[slovene]{babel}
\usepackage[cp1250]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{url}
%\usepackage[normalem]{ulem}
\usepackage[dvipsnames,usenames]{color}

% ne spreminjaj podatkov, ki vplivajo na obliko strani
\textwidth 15cm
\textheight 24cm
\oddsidemargin.5cm
\evensidemargin.5cm
\topmargin-5mm
\addtolength{\footskip}{10pt}
\pagestyle{plain}
\overfullrule=15pt % oznaci predlogo vrstico


% ukazi za matematicna okolja
\theoremstyle{definition} % tekst napisan pokoncno
\newtheorem{definicija}{Definicija}[section]
\newtheorem{primer}[definicija]{Primer}
\newtheorem{opomba}[definicija]{Opomba}

\renewcommand\endprimer{\hfill$\diamondsuit$}


\theoremstyle{plain} % tekst napisan posevno
\newtheorem{lema}[definicija]{Lema}
\newtheorem{izrek}[definicija]{Izrek}
\newtheorem{trditev}[definicija]{Trditev}
\newtheorem{posledica}[definicija]{Posledica}


% za stevilske mnozice uporabi naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}


% ukaz za slovarsko geslo
\newlength{\odstavek}
\setlength{\odstavek}{\parindent}
\newcommand{\geslo}[2]{\noindent\textbf{#1}\hspace*{3mm}\hangindent=\parindent\hangafter=1 #2}


% naslednje ukaze ustrezno popravi
\newcommand{\program}{Finanèna matematika} % ime studijskega programa: Matematika/Finan"cna matematika
\newcommand{\imeavtorja}{Martin Praèek} % ime avtorja
\newcommand{\imementorja}{izr. prof. dr. Damjan Škulj} % akademski naziv in ime mentorja
\newcommand{\naslovdela}{Skriti markovski modeli v èasovnih vrstah}
\newcommand{\letnica}{2019} %letnica diplome


% vstavi svoje definicije ...




\begin{document}

% od tod do povzetka ne spreminjaj nicesar
\thispagestyle{empty}
\noindent{\large
UNIVERZA V LJUBLJANI\\[1mm]
FAKULTETA ZA MATEMATIKO IN FIZIKO\\[5mm]
\program\ -- 1.~stopnja}
\vfill

\begin{center}{\large
\imeavtorja\\[2mm]
{\bf \naslovdela}\\[10mm]
Delo diplomskega seminarja\\[1cm]
Mentor: \imementorja}
\end{center}
\vfill

\noindent{\large
Ljubljana, \letnica}
\pagebreak

\thispagestyle{empty}
\tableofcontents
\pagebreak

\thispagestyle{empty}
\begin{center}
{\bf \naslovdela}\\[3mm]
{\sc Povzetek}
\end{center}
% tekst povzetka v slovenscini
V povzetku na kratko opiši vsebinske rezultate dela. Sem ne sodi razlaga organizacije dela -- v katerem poglavju/razdelku je kaj, paè pa le opis vsebine.
\vfill
\begin{center}
{\bf Hidden Markov Models in Time Series}\\[3mm] % prevod slovenskega naslova dela
{\sc Abstract}
\end{center}
% tekst povzetka v anglescini
Prevod zgornjega povzetka v angle"s"cino.

\vfill\noindent
{\bf Math. Subj. Class. (2010):} navedi vsaj eno klasifikacijsko oznako -- dostopne so na \url{www.ams.org/mathscinet/msc/msc2010.html}  \\[1mm]
{\bf Klju"cne besede:} skriti markovski modeli, èasovne vrste, sluèajni proces navedi nekaj klju"cnih pojmov, ki nastopajo v delu  \\[1mm]
{\bf Keywords:}  hidden markov models, time series, angle"ski prevod klju"cnih besed
\pagebreak


% tu se zacne besedilo seminarja
\section{Uvod}
Skriti markovski modeli so modelacijsko orodje, ki nam omogoèa zelo široko uporabo. V mojem diplomskem seminarju se bom najbolj posvetil uporabi v finanèni analizi.  
V prvem poglavju se bom posvetil razliènim naèinom uporabe skritih markovskih modelov, v drugem pa bom natanèno opisal matematièno ozadje le teh. Na koncu bom natanèno predstavil še lasten primer uporabe, kjer si bom pomagal z razliènimi programskimi jeziki in okolji, od Mathematice do R.




% slovar
\section*{Slovar strokovnih izrazov}

\geslo{Gaussova mešanica}{Gaussova mešanica je porazdelitvena funkcija z gostoto, ki jo lahko zapišemo kot tehtano povpreèje normalnih gostot; $f(x) = \sum_{j = 1}^{M}{c_{j}} N(x;\mu_{j}, \sigma_{j}^2)$. Veljati mora še, da je $\sum_{j = 1}^{M}{c_{j}} = 1$ in da je $N(x;\mu_{j}, \sigma_{j}^2)$ gostota normalne porazdelitvene funkcije.} \\
\geslo{Akaikov informacijski kriterij}{}
%
\pagebreak
\section{Markovski modeli}
Preden lahko zaènemo govoriti o skritih markovskih modelih, moram nekaj povedati o markovskih modelih.
Markovski modeli so modeli, kjer velja markovsko lastnost.
\definicija{Markovska lastnost}
Naj bo ${\displaystyle (\Omega ,{\mathcal {F}},\mathbb {P} )}$ verjetnostni prostor s filtracijo $({\displaystyle ({\mathcal {F}}_{s},\ s\in I)})$ za neko urejeno množico ${\displaystyle I}$. Naj bo $({\displaystyle (S,{\mathcal {S}})})$ merljiv prostor. Sluèajni proces X, merjen na $({\displaystyle (S,{\mathcal {S}})})$ X =${\displaystyle X=\{X_{t}:\Omega \to S\}_{t\in I}}$ prilagojen na filtracijo ima markovsko lastnost, èe za vsak ${\displaystyle A\in {\mathcal {S}}}$ in vsak ${\displaystyle s,t\in I}$ kjer je ${\displaystyle s<t}$, velja da je,

$${\displaystyle \mathbb {P} (X_{t}\in A\mid {\mathcal {F}}_{s})=\mathbb {P} (X_{t}\in A\mid X_{s}).}$$
Èe imamo ${\displaystyle S}$, ki je diskretna množica z diskretno sigma algebro in ${\displaystyle I=\mathbb {N} } I={\mathbb {N}}$, je lahko formulirano tudi kot:
$${\displaystyle \mathbb {P} (X_{n}=x_{n}\mid X_{n-1}=x_{n-1},\dots ,X_{0}=x_{0})=\mathbb {P} (X_{n}=x_{n}\mid X_{n-1}=x_{n-1}).} $$

Markovska lastnost je torej lastnost stohastiènega procesa, da je njegova vrednost v èasu $t$ odvisna le od njegove vrednosti v èasu $t-1$.
\\
Naravno se nam postavi vprašanje, zakaj je markovska lastnost koristna. Izkaže se, da nam omogoèa reševanje problemov, ki jih drugaèe v primernem èasu ne bi mogli rešiti. Model, ki ga lahko tako rešimo imenujemo Markov model.\\
Poznamo veè razliènih vrst Markovih modelov, ki jih lahko razdelimo med $4$ podkategorije.


\begin{table}[h!]
	\begin{tabular}{lcc}
		& V celoti opazovan & Le delno opazovan             \\
		\multicolumn{1}{c}{Avtonomen} & Markovska veriga                & Skriti markovski model                      \\
		Kontorliran                   & Markovski proces odloèanja      & Delno opazovalen proces odloèanja
	\end{tabular}
\end{table}
V mojem diplomskem delu se bom ukvarjal s skritimi markovskimi modeli.
\pagebreak
\section{Skriti markovski modeli}
Skriti markovski model je statistièni markovski model, kjer predpostavljamo, da je modelirani sistem markovski proces z skritimi stanji.\\
Gre torej za tip modela, kjer lahko razberemo rezultat, ne moremo pa ugotoviti, kakšna je bila funkcija, ki nam ga je dala. 

\subsection{Zahteve za model}
Kot vsak matematièni model, ima tudi skriti markovski model svoje zahteve.
\begin{enumerate}
	\item Prva zahteva je, da lahko rezultate, ki jih model producira, opazujemo v ekvidistanènih èasih, torej je razlika med dvema poljubnima zaporednima èasoma opazovanja $t-1$ in $t$ vedno enaka, na primer $d$. Rezulate imenujemo signali ali opazovanja.
	\item V vsakem izmed èasov $t$ je sistem lahko v enem izmed $N$ stanj. Ta stanja so $S_{1}, S_{2}, \ldots, S_{N}$. Vsako izmed stanj $S_{i}$ je sluèajna spremenjlivka, ki je lahko zvezna ali diskretna, a njenega porazdelitvenega zakona ne poznamo. Ta stanja so v èasu $t$ neznana. Stanja so torej doloèena s sluèajnimi spremenljivkami $S_{i}$, ki so lahko diskretne ali zvezne, njihove porazdelitve pa ne poznamo.
	\item  V vsakem izmed èasov $t$ ne vemo, v kakšnem stanju se nahajamo, vemo le, kakšni so rezultati našega procesa v tem èasu. Zato potrebujemo dodatni sluèajni proces ${\displaystyle Q=(Q_{t})_{t = 1,2, \ldots }}$, ki nam pove, v kakšnem stanju je sistem v èasu $t$. Tako velja, da je signal $O_{t}$ dan s stanjem sluèajnega procesa $Q$, v odvistnosti od gostote verjetnostne porazdelitve $b_{j}(O_{t})$. Signali našega procesa so torej rezultati sluèajnega procesa $Q$.
	\item Vektor verjetnosti zaèetnih stanj je oznaèen z $\Pi$, in vsota njegovih elementov je enaka $1$.
	\item V vsakem èasu velja, da bodisi sistem spremeni svoje stanje, bodisi ostane isto. Verjetnost prehoda v vsako stanje je doloèena s prehodnimi verjetnostmi, podanimi v matriki $A^{t}$, kjer verjetnost prehoda iz stanja $i$ v èasu $t$ v stanje $j$ v èasu $t+1$ simbolizira element $a_{ij}^{t}$. 
	\item Vsota vsakega stolpca vsake matrike $A^{t}$ je enaka $1$.
	\item  Ker govorimo o markovskem modelu, bo stanje v èasu $t+1$ odvisno le od stanja v èasu $t$, ne pa od celotne zgodovine.
\end{enumerate}
 \ \\
S temi predpostavkami bi lahko zmodelirali, a jih imamo še nekaj, ki nam sam model še precej olajšajo.
\begin{enumerate}
	\item Predpostavimo lahko, da so vse prehodne matrike $A^{t}$ enake, torej neodvisne od èasa $t$. Enolièno prehodno matriko torej lahko oznaèimo $A$. 
	\item Signal v èasu $t$ je odvisen le od stanja modela v tem èasu; torej so sluèajne spremenljivke v èasu $t$ odvisne le od tega.
\end{enumerate}
 \  \\
Naši signali so torej odvisni od stanja $S$. Ta stanja nam vrnejo rezultat v odvisnosti od verjetnostnih porazdelitev podanih z razliènimi parametri. Le te predstavimo kot mešanice normalnih porazdelitev in jih imenujemo tudi Gaussove mešanice.
Gaussovo mešanico definiramo kot tehtano povpreèje normalnih porazdelitvenih funkcij, glej slovar strokovnih izrazov. Vsako stanje $i$ iz nabora vseh stanj je torej podano z porazdelitveno gostoto $$b_{i} = \sum_{j = 1}^{M}{c_{ij}} N(x;\mu_{j}, \sigma_{j}^2)$$.\\
\\
Gaussove mešanice so primerne, ker lahko zelo dobro aproksimirajo vsako konèno zvezno porazdelitev, poleg tega pa se znebimo tveganja, ki nam ga v tem modelu predstavlja normalna porazdelitev. To tveganje predstavlja dejstvo, da je normalna porazdelitev simetrièna glede na njeno upanje; to pa pogosto ne drži za procese v realnem svetu, na primer za zaslužke v finanènem poslovanju.\\
Tako potrebujemo za sestavo modela še:
\begin{itemize}
	\item število mešanic $M$
	\item matriko $C$, ki predstavlja koeficiente $c_{ij}$, ki so faktorji v Gaussovi mešanici,
	\item matrika $\Gamma$, kjer $\mu_{ij}$ predstavlja prièakovano vrednost mešanice $j$ v stanju $i$,  ter
	\item matriko $\Sigma$, kjer $\sigma_{ij}$ predstavlja varianco mešanice $j$ v stanju $i$.
\end{itemize}
\   \\
\subsection{Priprava in trening modela}\label{trening}
Ko vemo kaj potrebujemo, lahko zaènemo s pripravo naših modelov. Preden zaènemo s doloèanjem parametrov našega modela, moramo najprej dobro preuèiti naš problem, saj je to znanje kljuèno za dobro reševanje problema. Kot prvo stvar moramo najprej izbrati set podatkov, na katerem se bo potekal tako imenoval trening sistema. Ti podatki morajo biti zbrani na na podobni enoti; èe na bo na primer zanimale cena delnice podjetja, ki se ukvarja z predelavo pomaranènega soka, za trening ne bomo vzeli cene delnic metalurškega podjetja. 
\\ Podatke, ki smo jih zbrali moramo nato urediti; doloèimo èasovne trenutke z enakim razponom, kot ga želimo imeti v našem modelu, in v teh trenutkih $t \in (1,\ldots, T)$ doloèimo opazovanja $O$. 
\\ Ko imamo podatke zbrane, moramo najprej doloèiti število stanj $N$ ter število mešanic $M$. Tu gre tudi za kljuèna problema priprave skritega markovskega modela.
\\V praksi število stanj vèasih doloèimo z potrebno aplikacijo, kot na primer v \ref{Biologija}, oziroma z vizualnim ogledom toèk grafa iz zgodovinskih podatkov.\\
Vèasih pa lahko to doloèimo preko Akaikovega informacijskega kriterija, ki primerja veè razliènih modelov, ki lahko doloèijo $N$ in izbere najboljšega. 
\\ Za pravilno doloèitev števila mešanic $M$ pa navadno izberemo razvršèanjem v skupine ($k$-mean clustering). 
\\
Ostale parametre navadno oznaèimo z $\lambda$ $=$ $(\Pi,A,C,\Gamma,\Sigma)$. Cilj treninga modela je, da parametre nastavimo tako, da bo verjetnost, da so bila vsa opazovanja doloèena tudi s strani modela najveèja, $P(O|\lambda)$. Pri tem mora veljati, da sta število mešanic $M$ ter število stanj $N$ že znana.
\\Naravno se pojavi vprašanje, kako to doloèimo. Izkaže se, da je uèinkovit sistem $naprej - nazaj$. Doloèimo spremenljivki $naprej$ $\alpha_{t}(i)$, ki predstavlja verjetnost, da so se zgodila opazovanja od $o$ do $t$ in stanje $i$, ter $nazaj$ $\beta_{t}(i)$, ki predstavlja verjetnost, da se bodo zgodila opazovanja od $t$ do $T$ pri stanju $i$. Definirati pa rabimo le eno izmed njiju, saj gre pri $\beta_{t}(i)$ le alternativno metodo za $\alpha_{t}(i)$.
$\alpha_{t}(i)$ definiramo kot:
$$\alpha_{1}(i) = \pi_{i}b_{i}(O_{1}) = \pi_{i}b_{i} = \sum_{k = 1}^{M}{c_{ik}} N(O_{1};\mu_{j}, \sigma_{j}^2)$$
Naslednje $\alpha_{t+1}(j)$ lahko nato induktivno izraèunamo kot 
$$\alpha_{t+1}(j) = b_{j}(O_{t+1})\sum_{i=1}^{N}{\alpha_{t}(i)a_{ij}}$$
Iz tega sledi, da je $P(O|\lambda) = \sum_{i=1}^{N}{\alpha_{T}(i)}$. Na enak naèin izraèunamo tudi $\beta_{t}(i)$, ki ni nujen za izraèun  $P(O|\lambda)$, vendar ga nujno rabimo za trening našega modela.
\\
\ \\
Skozi celotno toèko \ref{trening} smo se obnašali, kot da lahko vse podatke iz $\lambda$ kar izraèunamo. Vendar to ne drži v popolnosti. Res, ko je enkrat $naprej-nazaj$ vzpostavljen, se nam dozdeva, da gre le še za numerièno operacijo. Vendar težave nastopijo preden naš postopek zaène z delom. Problem nastopi ko moramo v naprej oceniti zaèetne parametre $\lambda$. \\
Ne poznamo analitiènega naèina, kako bi lahko ta problem rešili, vendar lahko najdemo tak $\lambda$, da lahko našo verjetnost $P(O|\lambda)$ lokalno maksimiziramo.Tu lahko uporabimo razliène algoritme, najbolj pa je uporabljen Baum-Welchov.\\
Pri tem moramo vsak paramater $\lambda$ najprej pogledati posebej. Izkaže se, da zaèetni vrednosti za prehodno matriko $A$ in zaèetni vektor $\Pi$ nista pomembni, èe nista ravno nièelna. Za $\Pi$ tako lahko vzamemo vektor, kjer so vse vrednosti enake $1/N$, kjer je $N$ število stanj.
\\
Veè problemov nam povzroèajo tri postavke z zvezno porazdelitivjo, to je $C$, $\Sigma$ in $\Gamma$. Dobra zaèetna ocena le teh je nujna za kakovost modela. Tudi tu se izkaže, da lahko, podobno kot pri oceni števila mešanic $M$ ozremo na razvršèanjem v skupine. Ta postopek nam sicer ne da globalnega minimuma, ki bi ga mogoèe lahko dobili na drugaèen naèin, a se v praksi izkaže za dobrega. Za $C$ to pomeni, da bo element $c_{ij}$ $=$ $1/k$ za vsak par $ij$, kjer je $k$ število šopov povpreèij. Prièakovane vrednosti in variance nato pridobimo iz vrednosti teh šopov povpreèij.
\ \\
Z znanimi zaèetnimi vrednostmi se lahko spustimo v maksumiziranje $P(O|\lambda)$. Osnovati moramo zaporedje $\lambda$ = $\lambda_{t}, t \in {0,\ldots}$, da bo veljalo  $$P(O|\lambda_{i+1})  \geqslant P(O|\lambda_{i})$$
Za to zaporedje velja, da konvergira k lokalnemu maksimumu.\\

\subsection{Natanèna doloèitev zaèetnih vrednosti}
Zanima nas torej, kako bomo osnovali zaporedje $\lambda$, ki ga potrebujemo za maksimizacijo $P(O|\lambda)$. Izkaže se, da za maksimizacijo le tega potrebujemo še nekaj dodatnih formul, ki so rezultat Baum-Welchovega algortma. Z njimi 		 definiramo veèfazni iterativni proces, pri katerem popravljamo vrednosti parametrov do konvergence.\\
Kot prvo potrebujemo $\xi_{t}(i,j)$, ki nam pove verjtnost, da smo v èasu $t$ v stanju $i$ in v èasu $t+1$ v stanju $j$.
$$\xi_{t}(i,j) = \frac{\alpha_{t}(i)a_{ij}b_{j}(O_{t})\beta_{t+1}(j)}{P(O|\lambda)}.$$
Vsota $\sum_{t=1}^{N}{\xi_{t}}$ nam pove prièakovano število prehodov iz stanja $i$ v stanje $j$.\\
Druga je $\gamma_{t}(i)$, ki nam pove verjetnost, da smo v èasu $t$ v stanju $i$.
$$\gamma_{t}(i) = \frac{\alpha_{t}(i)\beta_{t}(i)}{P(O|\lambda)}$$
Vsota $\sum_{t=1}^{N}{\gamma_{t}(i)}$ nam pove prièakovano število prehodov iz stanja $i$.\\
Zadnja, tretja pa je $\gamma_{t}(j,k)$, ki nam pove verjetnost, da smo v èasu $t$ v stanju $j$, in $k$ta mešanica predstavlja $O_{t}$. 
$$ \gamma_{t}(j,k) =\gamma_{t}(j)\frac{c_{jk} N(x;\mu_{jk}, \sigma_{jk}^2)}{\sum_{m = 1}^{M}{c_{jm}} N(x;\mu_{jm}, \sigma_{jm}^2)}$$
\ \\
Iz teh podatkov lahko nato popravimo izraèun elementov:
$$ \overline{\Pi_{i}} = \gamma_{1}(i)$$
$$ \overline{a_{ij}} = \frac{\sum_{t=1}^{T-1}{\xi_{t}}}{\sum_{t=1}^{T-1}{\gamma_{t}(i)}}$$
$$\overline{c_{jk}}= \frac{\sum_{t=1}^{T}{\gamma_{t}(j,k)}}{\sum_{t=1}^{T}\sum_{m=1}^{M}{\gamma_{t}(j,m)}}$$
$$\overline{\mu_{jk}}=\frac{\sum_{t=1}^{T}{\gamma_{t}(j,k)}O_{t}}{\sum_{t=1}^{T}{\gamma_{t}(j,k)}} $$
Ko je to doloèeno, nam zaporedje $\lambda$, doloèenih s temi parametri da lokalni maksimum. 
\subsection{Trenutno stanje}
Zadnja stvar, ki jo moramo narediti, preden zaènemo z doloèanjem prihodnjih stanj je doloèitev trenutnega stanja gospodarstva, torej stanja v zadnjem èasu, na katerem je naš model treniral.
Za doloèitev le tega uporabimo tako imenovani Viterbijev algoritem. Le ta je oblikovan tako, da nam vrne zaporedje $Q$, ki maksimizira $P(Q|O,\lambda)$.\\
Za delo s tem algoritmom moramo definirati $\delta_{t}(i)$, ki za vsako stanje $i$ vrne najveèjo verjetnost vzdolž poti v èasu $t$. Prek $\delta_{t}(i)$ nato induktivno izvedemo algoritem.\\
Viterbijev algoritem nam vrne $p*$, ki je najveèja verjetnost in $q_{T}*$, ki nam pove stanje v èasu $T$, ki nam to verjetnost vrne.
\pagebreak
\section{Uporaba}
Skriti markovski modeli so zelo široko uporabno matematièno orodje za modeliranje. Naèini uporabe se zelo razlikujejo in gredo od zelo bioloških do ekonomskih. Ekonomskim, torej tistim, ki jih delujejo kot napovedovalci cen vrednostnih papirjev v prihodnosti, se bom najglobje posvetil v nasldenjem delu, zato sedaj raje poglejmo ostale naèine uporabe.



\subsection{Procesiranje govora}

Ena najbolj široko uporabljenih naèinov uporabe pa je procesiranje govora za posamiène glasovne enote. Gre za sistem, kjer želimo prepoznati posamiène izgovorjene besede, ne moremo pa ga uporabiti za splošen govor. Ideja tega algoritma je razviti najboljše možne aproksimacijske algoritme, da lahko skriti markovski model filtrira nakljuène šume, zvoke na najboljši možen naèin. \\
Da bomo procesiranje lahko zmodelirali moramo najprej doloèiti slovar glasov, iz katerega vemo, da bo glas prišel. Tu uporabljam besedo glas, ker ni nujno, da bo prepoznan glas dejansko beseda; eden izmed glasov bi lahko bil tudi zgolj èrka A.
Velja omeniti tudi, da na ta naèin, le z nekaj adaptacijami, deluje tudi Siri.

\subsection{Uporaba v biologiji in biokemiji} \label{Biologija}
V zadnjem èasu se skrite markovske modele vedno veè uporablja tudi za modeliranje bioloških in biokemijskih procesov, med katerimi je najbolj  znan primer modeliranja proteinov v celièni membrani, ki ga bom malo opisal, poleg tega pa med drugim še za {\bf napovedovanje genov?}. \\
Èeprav lahko nekatere proteine in njihovo delovanje znotraj celiène membrane enostavno predstavimo, to ne velja za vse. Tako imenovani $\beta$
-\textbf{barrel} membranski proteini zahtevajo veè dela. Da bi ugotovili njihovo delovanje in ga primerjali z delovanjem v vodi topnih proteinov so Bagos, Liakoupos et al. razvili model ki to naredi. \\
Z delom potem nadaljujemo podobno kot pri procesiranju govora, le da je naš zaèetni slovar tu dolg $20$ znakov, toliko kolikor je aminokislin.
\\
\ \\
\ \\
Naslednjim trem kategorijam se do sedaj nisem pozorno posvetil.
\subsection{Napovedovanje prevoza}
Gre za sistem, kjer želimo za prihodnost napovedati, koliko vozil bo neko prometno infrastrukturo v prihodnosti.
%Preberi še kakšen vir

\subsection{Prepozavanje lastnoroène pisave}
%dodatni viri

\subsection{Kriptoanaliza}
S tem se je ukvarjajo razlièni storokovnjaki.
%novi viri, zbrani, preberi
\pagebreak
\section{Uporaba v èasovnih vrstah}
Temu delu se bom posvetil po kratki predstavitvi.

% seznam uporabljene literature
\pagebreak
\section{Praktièni primer}
\pagebreak
\bibliographystyle{unsrt}
\bibliography{roman,hmmvir1}
\bibitem{macdonald1997hidden}
\bibitem{roman}

\end{document}

